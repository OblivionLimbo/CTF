# where are the robots

Can you find the robots? 

`https://jupiter.challenges.picoctf.org/problem/56830/` ([link]https://jupiter.challenges.picoctf.org/problem/56830/) or http://jupiter.challenges.picoctf.org:56830

This uses a well known .txt file known as robots.txt, if we curl the website and add /robots.txt at the end we get something interesting. 

`curl http://jupiter.challenges.picoctf.org:56830/robots.txt` gives us the response: 
```
User-agent: *
Disallow: /1bb4c.html
```

Let's check out this /lbb4c.html:

`curl http://jupiter.challenges.picoctf.org:56830/1bb4c.html`

and we get the webpage, containing the flag: 

```html
<!doctype html>
<html>
  <head>
    <title>Where are the robots</title>
    <link href="https://fonts.googleapis.com/css?family=Monoton|Roboto" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="style.css">
  </head>
  <body>
    <div class="container">
      
      <div class="content">
	<p>Guess you found the robots<br />
	  <flag>picoCTF{ca1cu1at1ng_Mach1n3s_1bb4c}</flag></p>
      </div>
      <footer></footer>
  </body>
</html>

```